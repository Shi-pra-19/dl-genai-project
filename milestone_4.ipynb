{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 115439,
          "databundleVersionId": 13800781,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shi-pra-19/dl-genai-project/blob/main/milestone_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Milestone 4 ‚Äî Sequence Modeling with LSTM and GRU\n",
        "\n",
        "This milestone introduces **deep learning models (LSTM / GRU)** that are specifically designed to capture the **order and contextual relationships** between words in a sequence.\n",
        "\n",
        "---\n",
        "\n",
        "##  Suggested Readings\n",
        "- [LSTM](https://docs.pytorch.org/docs/stable/generated/torch.nn.GRU.html)\n",
        "- [GRU](https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è Instructions\n",
        "\n",
        "Use the **constants and helper functions** provided in the next cell to answer all **Milestone-4 questions**.\n",
        "\n",
        "Perform the following tasks on the **training dataset** provided as part of the Kaggle competition:\n",
        "\n",
        "üîó **Competition Link:**  \n",
        "[2025-Sep-DL-Gen-AI-Project](https://www.kaggle.com/competitions/2025-sep-dl-gen-ai-project)\n"
      ],
      "metadata": {
        "id": "e2ogIMAMt4VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "naJr2EGft4VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:22:49.567144Z",
          "iopub.execute_input": "2025-10-24T17:22:49.567471Z",
          "iopub.status.idle": "2025-10-24T17:22:49.573706Z",
          "shell.execute_reply.started": "2025-10-24T17:22:49.567448Z",
          "shell.execute_reply": "2025-10-24T17:22:49.572570Z"
        },
        "id": "JbLUKraxt4VO"
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set seeds and Constants"
      ],
      "metadata": {
        "id": "4J6MM3M4t4VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "DATA_SEED = 67\n",
        "TRAINING_SEED = 1234\n",
        "MAX_LEN = 50\n",
        "BATCH_SIZE = 64\n",
        "EMB_DIM = 100\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 5\n",
        "\n",
        "random.seed(DATA_SEED)\n",
        "np.random.seed(DATA_SEED)\n",
        "torch.manual_seed(DATA_SEED)\n",
        "torch.cuda.manual_seed(DATA_SEED)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:24:38.648298Z",
          "iopub.execute_input": "2025-10-24T17:24:38.648610Z",
          "iopub.status.idle": "2025-10-24T17:24:38.656492Z",
          "shell.execute_reply.started": "2025-10-24T17:24:38.648588Z",
          "shell.execute_reply": "2025-10-24T17:24:38.655516Z"
        },
        "id": "FziGMCfXt4VO"
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Vocab"
      ],
      "metadata": {
        "id": "WdF4Ds3-t4VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#data_path=     # enter your data path here\n",
        "df =   pd.read_csv(\"/content/train.csv\")        # read it and store it in df"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:37:54.421622Z",
          "iopub.execute_input": "2025-10-24T16:37:54.422106Z",
          "iopub.status.idle": "2025-10-24T16:37:54.474780Z",
          "shell.execute_reply.started": "2025-10-24T16:37:54.422083Z",
          "shell.execute_reply": "2025-10-24T16:37:54.473840Z"
        },
        "id": "SKh8SkoYt4VP"
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train df into train_df(80%) and test_df (20%) use seed\n",
        "# ------------------- write your code here -------------------------------\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=DATA_SEED)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:40:42.943254Z",
          "iopub.execute_input": "2025-10-24T16:40:42.944416Z",
          "iopub.status.idle": "2025-10-24T16:40:42.960903Z",
          "shell.execute_reply.started": "2025-10-24T16:40:42.944346Z",
          "shell.execute_reply": "2025-10-24T16:40:42.959758Z"
        },
        "id": "Yrv4U9a7t4VP"
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "# create a simple space-based tokenizer.\n",
        "# ------------------- write your code here -------------------------------\n",
        "def tokenize(text):\n",
        "    return text.split()\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:44:17.602830Z",
          "iopub.execute_input": "2025-10-24T16:44:17.603704Z",
          "iopub.status.idle": "2025-10-24T16:44:17.608521Z",
          "shell.execute_reply.started": "2025-10-24T16:44:17.603674Z",
          "shell.execute_reply": "2025-10-24T16:44:17.607566Z"
        },
        "id": "HJdeRv_ht4VP"
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c6fa82a"
      },
      "source": [
        "token_counter = Counter()\n",
        "for text in train_df['text']:\n",
        "    token_counter.update(tokenize(text))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train and val dataloaders"
      ],
      "metadata": {
        "id": "BBR5zOYPt4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#----------------------------- DON'T CHANGE THIS --------------------------\n",
        "specials = ['<unk>', '<pad>']\n",
        "min_freq = 2\n",
        "vocab_list = specials + [token for token, freq in token_counter.items() if freq >= min_freq]\n",
        "word2idx = {token: i for i, token in enumerate(vocab_list)}\n",
        "UNK_IDX = word2idx['<unk>']\n",
        "PAD_IDX = word2idx['<pad>']\n",
        "def text_pipeline(text):\n",
        "    \"\"\"Converts text to a list of indices using the word2idx dict.\"\"\"\n",
        "    tokens = tokenize(text)\n",
        "    return [word2idx.get(token, UNK_IDX) for token in tokens]\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, dataframe):\n",
        "        self.texts = dataframe['text'].values\n",
        "        self.labels = dataframe[['anger', 'fear', 'joy', 'sadness', 'surprise']].values.astype(np.float32)\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_text, _labels) in batch:\n",
        "        label_list.append(_labels)\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)[:MAX_LEN]\n",
        "        text_list.append(processed_text)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.float32)\n",
        "    text_list = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
        "    if text_list.shape[1] < MAX_LEN:\n",
        "        pad_tensor = torch.full(\n",
        "            (text_list.shape[0], MAX_LEN - text_list.shape[1]),\n",
        "            PAD_IDX,\n",
        "            dtype=torch.int64\n",
        "        )\n",
        "        text_list = torch.cat((text_list, pad_tensor), dim=1)\n",
        "\n",
        "    return text_list, label_list\n",
        "\n",
        "# Create train and val dataloaders\n",
        "# ------------------- write your code here -------------------------------\n",
        "train_dataset = EmotionDataset(train_df)\n",
        "val_dataset = EmotionDataset(test_df)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:21:56.467351Z",
          "iopub.execute_input": "2025-10-24T17:21:56.467673Z",
          "iopub.status.idle": "2025-10-24T17:21:56.483356Z",
          "shell.execute_reply.started": "2025-10-24T17:21:56.467652Z",
          "shell.execute_reply": "2025-10-24T17:21:56.482391Z"
        },
        "id": "DTFAKSgPt4VQ"
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Q0KEXh1wt4VQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What are the vocabulary size, padding token index, and unknown token index for the above dataset?"
      ],
      "metadata": {
        "id": "nwq9_wK4t4VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "VOCAB_SIZE = len(vocab_list)\n",
        "print(f\"Vocabulary Size: {VOCAB_SIZE}\")\n",
        "print(f\"Padding Token Index (PAD_IDX): {PAD_IDX}\")\n",
        "print(f\"Unknown Token Index (UNK_IDX): {UNK_IDX}\")\n",
        "#-------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:57:14.541326Z",
          "iopub.execute_input": "2025-10-24T16:57:14.541611Z",
          "iopub.status.idle": "2025-10-24T16:57:14.546909Z",
          "shell.execute_reply.started": "2025-10-24T16:57:14.541593Z",
          "shell.execute_reply": "2025-10-24T16:57:14.546053Z"
        },
        "id": "InMGo-zwt4VQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1216d14-7cc3-4398-b548-a477e6f475a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary Size: 2\n",
            "Padding Token Index (PAD_IDX): 1\n",
            "Unknown Token Index (UNK_IDX): 0\n"
          ]
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2.What are the indices for the words \"happy\", \"alone\", and \"sad\" in the vocabulary?"
      ],
      "metadata": {
        "id": "W2s9owsCt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "happy_idx = word2idx.get('happy', UNK_IDX)\n",
        "alone_idx = word2idx.get('alone', UNK_IDX)\n",
        "sad_idx = word2idx.get('sad', UNK_IDX)\n",
        "\n",
        "print(f\"Index for 'happy': {happy_idx}\")\n",
        "print(f\"Index for 'alone': {alone_idx}\")\n",
        "print(f\"Index for 'sad': {sad_idx}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T16:58:00.297702Z",
          "iopub.execute_input": "2025-10-24T16:58:00.298047Z",
          "iopub.status.idle": "2025-10-24T16:58:00.304387Z",
          "shell.execute_reply.started": "2025-10-24T16:58:00.298024Z",
          "shell.execute_reply": "2025-10-24T16:58:00.302684Z"
        },
        "id": "LVNCVQADt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "676bffa6-6be4-4937-b293-9782a8e86745"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index for 'happy': 1578\n",
            "Index for 'alone': 2525\n",
            "Index for 'sad': 885\n"
          ]
        }
      ],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:28:15.777599Z",
          "iopub.execute_input": "2025-10-24T17:28:15.777917Z",
          "iopub.status.idle": "2025-10-24T17:28:15.784144Z",
          "shell.execute_reply.started": "2025-10-24T17:28:15.777894Z",
          "shell.execute_reply": "2025-10-24T17:28:15.783195Z"
        },
        "id": "N83sTQ8At4VR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Get one batch to test shapes\n",
        "#take one batch as input here and store it in text_batch\n",
        "emb_layer = nn.Embedding(VOCAB_SIZE, EMB_DIM)\n",
        "embedded_batch = emb_layer(text_batch)\n",
        "\n",
        "# Simple LSTM layer Output Shape (Use constants defined in 2nd cell)\n",
        "lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "lstm_output, (hidden, cell) = lstm(embedded_batch)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:27.729908Z",
          "iopub.execute_input": "2025-10-24T17:43:27.730227Z",
          "iopub.status.idle": "2025-10-24T17:43:27.805156Z",
          "shell.execute_reply.started": "2025-10-24T17:43:27.730207Z",
          "shell.execute_reply": "2025-10-24T17:43:27.804231Z"
        },
        "id": "_6frZYdht4VR"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is the output shape of the Embedding layer?\n"
      ],
      "metadata": {
        "id": "j2m_SwrZt4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Output shape of the Embedding layer: {embedded_batch.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:30.395104Z",
          "iopub.execute_input": "2025-10-24T17:43:30.395595Z",
          "iopub.status.idle": "2025-10-24T17:43:30.401656Z",
          "shell.execute_reply.started": "2025-10-24T17:43:30.395569Z",
          "shell.execute_reply": "2025-10-24T17:43:30.400766Z"
        },
        "id": "h4C8uR4zt4VR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06eb08ec-ae53-4712-f600-0bb69f49cafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape of the Embedding layer: torch.Size([64, 50, 100])\n"
          ]
        }
      ],
      "execution_count": 22
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What will be output shape of simple LSTM layer"
      ],
      "metadata": {
        "id": "cQi8Xfhit4VR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Output shape of simple LSTM layer: {lstm_output.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:33.022293Z",
          "iopub.execute_input": "2025-10-24T17:43:33.022614Z",
          "iopub.status.idle": "2025-10-24T17:43:33.027754Z",
          "shell.execute_reply.started": "2025-10-24T17:43:33.022577Z",
          "shell.execute_reply": "2025-10-24T17:43:33.026937Z"
        },
        "id": "eL299270t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ae5424-1968-47c6-a186-fd1992c13a38"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape of simple LSTM layer: torch.Size([64, 50, 256])\n"
          ]
        }
      ],
      "execution_count": 23
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is the 'hidden' state shape from a simple LSTM?"
      ],
      "metadata": {
        "id": "2WyMjs-Pt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Hidden state shape from a simple LSTM: {hidden.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:43:37.314103Z",
          "iopub.execute_input": "2025-10-24T17:43:37.314400Z",
          "iopub.status.idle": "2025-10-24T17:43:37.319394Z",
          "shell.execute_reply.started": "2025-10-24T17:43:37.314379Z",
          "shell.execute_reply": "2025-10-24T17:43:37.318208Z"
        },
        "id": "SFu7xJO2t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3917d39f-3d34-4301-f308-e332df0987af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden state shape from a simple LSTM: torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 24
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. What is the 'hidden' state shape from a simple GRU?"
      ],
      "metadata": {
        "id": "P-_KR3gJt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru = nn.GRU(EMB_DIM, HIDDEN_DIM, batch_first=True)\n",
        "gru_output, gru_hidden = gru(embedded_batch)\n",
        "print(f\"Hidden state shape from a simple GRU: {gru_hidden.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:43.149832Z",
          "iopub.execute_input": "2025-10-24T17:48:43.150446Z",
          "iopub.status.idle": "2025-10-24T17:48:43.205538Z",
          "shell.execute_reply.started": "2025-10-24T17:48:43.150421Z",
          "shell.execute_reply": "2025-10-24T17:48:43.204760Z"
        },
        "id": "6zu5csBlt4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "429843b3-4aee-4c7e-8359-a212b19da7b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden state shape from a simple GRU: torch.Size([1, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 25
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. What is the 'output' tensor shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "9B1Aw1S3t4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bidirectional_lstm = nn.LSTM(EMB_DIM, HIDDEN_DIM, batch_first=True, bidirectional=True)\n",
        "bidirec_lstm_output, (bidirec_lstm_hidden, bidirec_lstm_cell) = bidirectional_lstm(embedded_batch)\n",
        "print(f\"Output tensor shape from a bidirectional LSTM: {bidirec_lstm_output.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:48:51.968585Z",
          "iopub.execute_input": "2025-10-24T17:48:51.968881Z",
          "iopub.status.idle": "2025-10-24T17:48:52.117279Z",
          "shell.execute_reply.started": "2025-10-24T17:48:51.968860Z",
          "shell.execute_reply": "2025-10-24T17:48:52.116302Z"
        },
        "id": "DsDrU39At4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f05ef7fd-5c72-4b00-db3e-0ed11458393c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor shape from a bidirectional LSTM: torch.Size([64, 50, 512])\n"
          ]
        }
      ],
      "execution_count": 26
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What is the 'hidden' state shape from a bidirectional LSTM?"
      ],
      "metadata": {
        "id": "3HoQngbQt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Hidden state shape from a bidirectional LSTM: {bidirec_lstm_hidden.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T17:53:18.625946Z",
          "iopub.execute_input": "2025-10-24T17:53:18.626297Z",
          "iopub.status.idle": "2025-10-24T17:53:18.632909Z",
          "shell.execute_reply.started": "2025-10-24T17:53:18.626276Z",
          "shell.execute_reply": "2025-10-24T17:53:18.631609Z"
        },
        "id": "dRX_2qu3t4VS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7b13207-2d0e-48b2-934b-a4d9473f8d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden state shape from a bidirectional LSTM: torch.Size([2, 64, 256])\n"
          ]
        }
      ],
      "execution_count": 27
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. Create 3 sequential models using the (Simple & Bidirectional)LSTM and Stacked GRU (2 layers)For all models, follow this(Embedding layer ‚Üí [LSTM / BiLSTM / Stacked GRU] ‚Üí Linear layer) architecture. What will be the training parameters in all 3 cases?(LSTM, BiLSTM, Stacked GRU)"
      ],
      "metadata": {
        "id": "wDxAlIkvt4VS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:18:21.974713Z",
          "iopub.execute_input": "2025-10-24T18:18:21.975047Z",
          "iopub.status.idle": "2025-10-24T18:18:21.980302Z",
          "shell.execute_reply.started": "2025-10-24T18:18:21.975020Z",
          "shell.execute_reply": "2025-10-24T18:18:21.979147Z"
        },
        "id": "Y1xjyI8et4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------- write your code here -------------------------------\n",
        "# Simple LSTM Model\n",
        "class SimpleLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
        "        # Use the hidden state from the last time step for classification\n",
        "        # hidden.shape is (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # For simple LSTM, it's (1, batch_size, hidden_size)\n",
        "        return self.fc(hidden.squeeze(0))\n",
        "\n",
        "simple_lstm_model = SimpleLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "print(f\"Simple LSTM Model Parameters: {sum(p.numel() for p in simple_lstm_model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# Bidirectional LSTM Model\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim) # Multiply by 2 for bidirectional\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        lstm_output, (hidden, cell) = self.lstm(embedded)\n",
        "        # Concatenate the hidden states from the forward and backward directions\n",
        "        # hidden.shape is (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # For bidirectional, it's (2, batch_size, hidden_size)\n",
        "        hidden_combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        return self.fc(hidden_combined)\n",
        "\n",
        "bilstm_model = BiLSTM(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "print(f\"Bidirectional LSTM Model Parameters: {sum(p.numel() for p in bilstm_model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# Stacked GRU Model (2 layers)\n",
        "class StackedGRU(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        gru_output, hidden = self.gru(embedded)\n",
        "        # Use the hidden state from the last layer for classification\n",
        "        # hidden.shape is (num_layers * num_directions, batch_size, hidden_size)\n",
        "        # For stacked GRU, it's (num_layers, batch_size, hidden_size)\n",
        "        return self.fc(hidden[-1,:,:])\n",
        "\n",
        "stacked_gru_model = StackedGRU(VOCAB_SIZE, EMB_DIM, HIDDEN_DIM, OUTPUT_DIM, num_layers=2)\n",
        "print(f\"Stacked GRU Model (2 layers) Parameters: {sum(p.numel() for p in stacked_gru_model.parameters() if p.requires_grad)}\")\n",
        "#------------------------------------------------------------------------"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-24T18:21:28.488340Z",
          "iopub.execute_input": "2025-10-24T18:21:28.488678Z",
          "iopub.status.idle": "2025-10-24T18:21:28.530039Z",
          "shell.execute_reply.started": "2025-10-24T18:21:28.488657Z",
          "shell.execute_reply": "2025-10-24T18:21:28.529049Z"
        },
        "id": "jN6AU6wjt4VT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f66894-16c3-4965-fd62-a9584820635e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simple LSTM Model Parameters: 940877\n",
            "Bidirectional LSTM Model Parameters: 1308749\n",
            "Stacked GRU Model (2 layers) Parameters: 1243981\n"
          ]
        }
      ],
      "execution_count": 28
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "cM_78-Est4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "7JuXKBDLt4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q11. Compare the total training time for your best sequential model against the simple averaging model from Milestone 3. How much longer (in minutes or percentage) did the more complex model (LSTM and GRU) take to train for the same number of epochs?"
      ],
      "metadata": {
        "id": "rKXe8Bcht4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "DMt4dVRbt4VT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q12. If you experimented with both LSTM and GRU models using the same hyperparameters, which one achieved a better peak Macro F1-score in your W&B logs?"
      ],
      "metadata": {
        "id": "vmHxpiLst4VT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ExtGs52Ct4Vc"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q13 Based on your experiments, what was the most impactful hyperparameter you tuned for your sequential model (e.g., learning rate, hidden size, number of layers, dropout rate)?"
      ],
      "metadata": {
        "id": "TA7mcOOKt4Vc"
      }
    }
  ]
}