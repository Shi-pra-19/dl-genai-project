{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17486,"status":"ok","timestamp":1762962395060,"user":{"displayName":"Shipra","userId":"05727147388801096449"},"user_tz":-330},"id":"iCFAwrEes8Cf","outputId":"35482f9f-c805-4fb7-e2d1-8ae8f168f5e2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nlpaug in /usr/local/lib/python3.12/dist-packages (1.1.11)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n","Requirement already satisfied: numpy>=1.16.2 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.0.2)\n","Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.2.2)\n","Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (2.32.4)\n","Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from nlpaug) (5.2.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (4.13.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from gdown>=4.0.0->nlpaug) (3.20.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2.0->nlpaug) (2025.2)\n","Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (3.4.4)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (3.11)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->nlpaug) (2025.10.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.2.0->nlpaug) (1.17.0)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (2.8)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug) (4.15.0)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.12/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug) (1.7.1)\n","<frozen runpy>:128: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["!pip install nlpaug nltk\n","!python -m nltk.downloader wordnet omw-1.4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptK3CSJFQ94D"},"outputs":[],"source":["import pandas as pd\n","import torch\n","from transformers import MarianMTModel, MarianTokenizer\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import nlpaug.augmenter.word as naw"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hl4Sdpv0SY-L"},"outputs":[],"source":["df = pd.read_csv('/content/train_clean.csv')\n","test = pd.read_csv('/content/test_clean.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fWzSlNfzTL8w"},"outputs":[],"source":["label_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n","\n","label_counts = df[label_cols].sum().sort_values(ascending=False)\n","\n","\n","label_percent = (label_counts / label_counts.sum() * 100).round(2)\n","\n","\n","label_table = pd.DataFrame({\n","    'Label': label_counts.index,\n","    'Count': label_counts.values,\n","    'Percentage (%)': label_percent.values\n","})\n","\n","print(label_table)\n"]},{"cell_type":"markdown","metadata":{"id":"Gg7rcVJQUb2q"},"source":["\n","\n","*   1x for 500 in sadness,suprise\n","* 1x for 800 in jou\n","\n","\n","\n","* anger 500 - 2000(3x)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFghvHwwZ0v2"},"outputs":[],"source":["def load_model(name):\n","    tok = MarianTokenizer.from_pretrained(name)\n","    model = MarianMTModel.from_pretrained(name)\n","    return tok, model\n","\n","en2hi_tok, en2hi_model = load_model(\"Helsinki-NLP/opus-mt-en-hi\")\n","hi2en_tok, hi2en_model = load_model(\"Helsinki-NLP/opus-mt-hi-en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_nvudyYecCUQ"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"USlZk5fBcC1B"},"outputs":[],"source":["def translate(texts, tokenizer, model, batch_size=32):\n","    all_out = []\n","    model = model.to(device)\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Translating\", leave=False):\n","        batch = texts[i:i+batch_size]\n","        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n","        with torch.no_grad():\n","            outputs = model.generate(**inputs, max_length=256)\n","        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n","        all_out.extend(decoded)\n","    return all_out\n","\n","def back_translate(texts, batch_size=64):\n","    mid = translate(texts, en2hi_tok, en2hi_model, batch_size)\n","    back = translate(mid, hi2en_tok, hi2en_model, batch_size)\n","    return back"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JUzJr2fDeI5s"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","\n","def augment(df):\n","    plan = {\n","        \"sadness\": {\"pick\": 500, \"times\": 1},\n","        \"surprise\": {\"pick\": 500, \"times\": 1},\n","        \"joy\": {\"pick\": 800, \"times\": 1},\n","        \"anger\": {\"pick\": 500, \"times\": 5},\n","    }\n","\n","    label_cols = ['anger','joy','surprise','fear','sadness']\n","    augmented_rows = []\n","    used_indices = set()\n","\n","    for label, cfg in plan.items():\n","        available = df[(df[label] == 1) & (~df.index.isin(used_indices))]\n","        if len(available) < cfg[\"pick\"]:\n","            print(f\"Not enough {label} samples ({len(available)}) - picking all available.\")\n","            subset = available\n","        else:\n","            subset = available.sample(cfg[\"pick\"], random_state=42)\n","\n","        used_indices.update(subset.index)\n","        texts = subset[\"text\"].tolist()\n","        print(f\"{label}: picked {len(texts)} → {cfg['times']}× Hindi back-translation\")\n","\n","        for i in range(cfg[\"times\"]):\n","            print(f\"Round {i+1}/{cfg['times']}\")\n","            aug_texts = back_translate(texts, batch_size=64)\n","\n","            new_df = subset.copy()\n","            new_df[\"text\"] = aug_texts\n","            new_df[\"id\"] = [f\"{label}_aug_{i}_{idx}\" for idx in range(len(new_df))]\n","            augmented_rows.append(new_df)\n","\n","\n","    df_aug = pd.concat(augmented_rows, ignore_index=True)\n","    combined_df = pd.concat([df, df_aug], ignore_index=True)\n","\n","    return combined_df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EmxKZyPgAHd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"663c45bd-f495-42c9-93c9-a27fe527f172"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["sadness: picked 500 → 1× Hindi back-translation\n","Round 1/1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["surprise: picked 500 → 1× Hindi back-translation\n","Round 1/1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["joy: picked 800 → 1× Hindi back-translation\n","Round 1/1\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Not enough anger samples (445) — picking all available.\n","anger: picked 445 → 5× Hindi back-translation\n","Round 1/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Round 2/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Round 3/5\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Round 4/5\n"]},{"output_type":"stream","name":"stderr","text":["Translating:   0%|          | 0/7 [00:00<?, ?it/s]"]}],"source":["df_augment = augment(df)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4bNy_bx6tShz"},"outputs":[],"source":["def augment_syn(df):\n","    plan = {\n","        \"sadness\": {\"pick\": 800, \"times\": 1},\n","        \"surprise\": {\"pick\": 800, \"times\": 1},\n","        \"joy\": {\"pick\": 1200, \"times\":1},\n","        \"anger\": {\"pick\": 500, \"times\": 5},\n","    }\n","\n","    label_cols = ['anger', 'joy', 'surprise', 'fear', 'sadness']\n","    augmented_rows = []\n","    used_indices = set()\n","\n","\n","    aug = naw.ContextualWordEmbsAug(\n","        model_path='bert-base-uncased',\n","        action='substitute',\n","        device='cuda'\n","    )\n","\n","    for label, cfg in plan.items():\n","        available = df[(df[label] == 1) & (~df.index.isin(used_indices))]\n","        if len(available) < cfg[\"pick\"]:\n","            print(f\"Not enough {label} samples ({len(available)}) - picking all available.\")\n","            subset = available\n","        else:\n","            subset = available.sample(cfg[\"pick\"], random_state=42)\n","\n","        used_indices.update(subset.index)\n","        texts = subset[\"text\"].tolist()\n","        print(f\"{label}: picked {len(texts)} → {cfg['times']}× contextual synonym augmentation\")\n","\n","        for i in range(cfg[\"times\"]):\n","            print(f\"Round {i+1}/{cfg['times']}\")\n","            aug_texts = aug.augment(texts)\n","\n","            new_df = subset.copy()\n","            new_df[\"text\"] = aug_texts\n","            new_df[\"id\"] = [f\"{label}_bert_syn_{i}_{idx}\" for idx in range(len(new_df))]\n","            augmented_rows.append(new_df)\n","\n","    df_aug = pd.concat(augmented_rows, ignore_index=True)\n","    combined_df = pd.concat([df, df_aug], ignore_index=True)\n","\n","    return combined_df"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"referenced_widgets":["72ae4b5b5a8d41fe860a75761657a499","4e5ba65ce72e4b19bce9bb6b9ca6d84d","5ebaa77c62d24b84b5abdb2bfac7e675","b1aa4ae2b67f4ba1b9024b17e1f72186","174cafc33b5f4ff6b706b57e7e23e734"]},"id":"9AW3pCwktYvj","outputId":"b36b2309-bf8c-4eb3-9169-f9efd52c30a8"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72ae4b5b5a8d41fe860a75761657a499","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4e5ba65ce72e4b19bce9bb6b9ca6d84d","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ebaa77c62d24b84b5abdb2bfac7e675","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b1aa4ae2b67f4ba1b9024b17e1f72186","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"174cafc33b5f4ff6b706b57e7e23e734","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The following layers were not sharded: bert.encoder.layer.*.attention.output.dense.bias, bert.encoder.layer.*.attention.self.key.bias, bert.encoder.layer.*.intermediate.dense.bias, cls.predictions.transform.LayerNorm.bias, bert.encoder.layer.*.attention.self.key.weight, bert.embeddings.token_type_embeddings.weight, bert.encoder.layer.*.output.dense.weight, bert.embeddings.LayerNorm.weight, bert.encoder.layer.*.output.LayerNorm.bias, cls.predictions.transform.dense.weight, bert.encoder.layer.*.attention.output.LayerNorm.bias, bert.encoder.layer.*.attention.self.value.weight, cls.predictions.transform.LayerNorm.weight, cls.predictions.decoder.bias, bert.embeddings.word_embeddings.weight, bert.encoder.layer.*.attention.self.value.bias, cls.predictions.decoder.weight, bert.encoder.layer.*.attention.output.LayerNorm.weight, bert.embeddings.LayerNorm.bias, bert.encoder.layer.*.attention.self.query.weight, bert.encoder.layer.*.attention.output.dense.weight, cls.predictions.transform.dense.bias, cls.predictions.bias, bert.embeddings.position_embeddings.weight, bert.encoder.layer.*.output.dense.bias, bert.encoder.layer.*.attention.self.query.bias, bert.encoder.layer.*.intermediate.dense.weight, bert.encoder.layer.*.output.LayerNorm.weight\n"]},{"name":"stdout","output_type":"stream","text":["sadness: picked 500 → 1× contextual synonym augmentation\n","Round 1/1\n","surprise: picked 500 → 1× contextual synonym augmentation\n","Round 1/1\n","joy: picked 800 → 1× contextual synonym augmentation\n","Round 1/1\n","anger: picked 500 → 5× contextual synonym augmentation\n","Round 1/5\n","Round 2/5\n","Round 3/5\n","Round 4/5\n","Round 5/5\n"]}],"source":["df_balanced = augment_syn(df_augment)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hdoe_Q9UgS6o","outputId":"caa76ad8-93d2-46a8-b7af-fe1851c44d28"},"outputs":[{"data":{"text/plain":["(13311, 8)"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["df_balanced.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"KeoyV61Jiz0X","outputId":"a471cc2b-6b56-4db1-bf42-dbc285719396"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>13309</td>\n","    </tr>\n","    <tr>\n","      <th>unique</th>\n","      <td>11529</td>\n","    </tr>\n","    <tr>\n","      <th>top</th>\n","      <td>Oh, I don't know... against the Bill of Power?</td>\n","    </tr>\n","    <tr>\n","      <th>freq</th>\n","      <td>5</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div><br><label><b>dtype:</b> object</label>"],"text/plain":["count                                              13309\n","unique                                             11529\n","top       Oh, I don't know... against the Bill of Power?\n","freq                                                   5\n","Name: text, dtype: object"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["df_balanced.text.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"sTcg7V4hjhe7"},"outputs":[],"source":["df_balanced = df_balanced.drop_duplicates(subset='text', keep='first')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"NCdFJLZLjjC7","outputId":"f11c32c0-00c1-4e8a-ddff-98579b222a5b"},"outputs":[{"data":{"text/plain":["(11530, 8)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["df_balanced.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"phlyO1uYjdTT"},"outputs":[],"source":["df_balanced.to_csv(\"augmented_data.csv\", index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"jasBWiC2jytX","outputId":"05462799-4792-4fab-869e-6177ac81d043"},"outputs":[{"name":"stdout","output_type":"stream","text":["      Label  Count  Percentage (%)\n","0      fear   6488           30.86\n","1     anger   4034           19.18\n","2   sadness   3755           17.86\n","3  surprise   3648           17.35\n","4       joy   3102           14.75\n"]}],"source":["label_cols = ['anger', 'fear', 'joy', 'sadness', 'surprise']\n","\n","label_counts = df_balanced[label_cols].sum().sort_values(ascending=False)\n","\n","\n","label_percent = (label_counts / label_counts.sum() * 100).round(2)\n","\n","\n","label_table = pd.DataFrame({\n","    'Label': label_counts.index,\n","    'Count': label_counts.values,\n","    'Percentage (%)': label_percent.values\n","})\n","\n","print(label_table)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"dR43oID3sERD","outputId":"f41feb44-7574-4a5b-fd04-4085e8ed4521"},"outputs":[{"name":"stdout","output_type":"stream","text":["Empty DataFrame\n","Columns: [count]\n","Index: []\n"]}],"source":["duplicates = df_balanced['text'].value_counts().to_frame('count')\n","duplicates = duplicates[duplicates['count'] > 1]\n","print(duplicates)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyOilP9uF8OSrWJ+y3Bg8jvH"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}